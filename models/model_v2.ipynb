{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e040ec1-8705-4f3e-a020-dc08409d03b8",
   "metadata": {},
   "source": [
    "## Same intial loading and cleaning steps ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4147e45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preparation Imports\n",
    "import cv2\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "file_name = '../data/wiki.mat'\n",
    "\n",
    "def matlab_datenum_to_date(matlab_dn):\n",
    "    return datetime.date.fromordinal(int(matlab_dn) - 366)\n",
    "\n",
    "def compute_wiki_age(photo_year, birth_datenum):\n",
    "    photo_date = datetime.date(photo_year, 7, 1)\n",
    "    birth_date = matlab_datenum_to_date(birth_datenum)\n",
    "    delta = photo_date - birth_date\n",
    "    return delta.days / 365.2425\n",
    "\n",
    "\n",
    "def process_matfile(file_name):\n",
    "    # Load the data\n",
    "    data = loadmat(file_name)['wiki'][0, 0]\n",
    "    data_dict = {key: data[key] for key in data.dtype.names}\n",
    "\n",
    "    # Convert data into filename-based dictionary\n",
    "    filename_dict = {}\n",
    "\n",
    "    for i in range(len(data_dict['full_path'][0])):\n",
    "        # Remove invalid faces\n",
    "        if np.isinf(data_dict['face_score'][0][i]) or not np.isnan(data_dict['second_face_score'][0][i]):\n",
    "            continue\n",
    "\n",
    "        filename = data_dict['full_path'][0][i][0]\n",
    "        filename_dict[filename] = compute_wiki_age(data_dict['photo_taken'][0][i], data_dict['dob'][0][i])\n",
    "\n",
    "    return filename_dict\n",
    "\n",
    "# Load filenames anda ssociated data\n",
    "data = process_matfile(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de848d31-42b3-4271-b295-9986f432db6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40216\n",
      "38.60996556567274\n",
      "40147\n",
      "1.3525260614523227 99.87337180092678\n"
     ]
    }
   ],
   "source": [
    "#some data cleaning to get rid of excessively large or negative values in the predicted age category\n",
    "print(len(data))\n",
    "mean_val = sum(data.values()) / len(data)\n",
    "print(mean_val)\n",
    "for key in list(data.keys()):\n",
    "    if data[key] > 100 or data[key] < 1:\n",
    "        del data[key]\n",
    "print(len(data))\n",
    "print(min(data.values()), max(data.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52c154cb-9def-4a6c-a7c5-d748c77d8d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images loaded: 40147\n"
     ]
    }
   ],
   "source": [
    "# Load in images\n",
    "images = []\n",
    "for i in data.keys():  \n",
    "    img = cv2.imread('../data/wiki_crop/'+i)\n",
    "    images.append(img)\n",
    "\n",
    "print(f\"Number of images loaded: {len(images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c1beabd-ca4a-4268-ab39-313b827a6e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resize all images (can change width and height to what we find to be best)\n",
    "new_images = []\n",
    "width = 200\n",
    "height = 200\n",
    "for image in images:\n",
    "    new_im = cv2.resize(image, (width,height), interpolation=cv2.INTER_LINEAR)\n",
    "    new_images.append(new_im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb9d6ee-68d7-4cd8-89d6-7181abe38f76",
   "metadata": {},
   "source": [
    "## new way to classify ages into bins of 5 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9483bb2-6f04-4b98-b0bb-63df96f5b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]\n",
    "labels = ['0-5 years', '5-10 years', '10-15 years', '15-20 years',\n",
    "              '20-25 years', '25-30 years', '30-35 years', '35-40 years',\n",
    "             '40-45 years', '45-50 years', '50-55 years', '55-60 years',\n",
    "              '60-65 years', '65-70 years', '70-75 years', '75-80 years', \n",
    "             '80-85 years', '85-90 years', '90-95 years', '95-100 years']\n",
    "\n",
    "def classify_age(age, edges, labels):\n",
    "    for i in range(len(edges) - 1):\n",
    "        if edges[i] <= age < edges[i + 1]:\n",
    "            return labels[i]\n",
    "    return None  \n",
    "\n",
    "classification = [classify_age(data[key], edges, labels) for key in data.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0964e931-582f-4e40-8170-c7a97fba9394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in to train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(new_images, classification, test_size = 0.8, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60bca587-84dd-4c09-9ae0-1c893c8f1fae",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 14.4 GiB for an array with shape (32118, 200, 200, 3) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Convert datasets\u001b[39;00m\n\u001b[0;32m      8\u001b[0m X_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(x_train, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m X_test \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfloat32\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Map labels to integers\u001b[39;00m\n\u001b[0;32m     12\u001b[0m y_train_int \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([label_to_int(lbl) \u001b[38;5;28;01mfor\u001b[39;00m lbl \u001b[38;5;129;01min\u001b[39;00m y_train], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 14.4 GiB for an array with shape (32118, 200, 200, 3) and data type float32"
     ]
    }
   ],
   "source": [
    "int_labels = sorted(set(y_train))\n",
    "label_to_int = {label: idx for idx, label in enumerate(int_labels)}\n",
    "\n",
    "def label_to_int(label):\n",
    "    return label_to_int[label]\n",
    "\n",
    "# Convert datasets\n",
    "X_train = np.array(x_train, dtype='float32')\n",
    "X_test = np.array(x_test, dtype='float32')\n",
    "\n",
    "# Map labels to integers\n",
    "y_train_int = np.array([label_to_int(lbl) for lbl in y_train], dtype='int32')\n",
    "y_test_int = np.array([label_to_int(lbl) for lbl in y_test], dtype='int32')\n",
    "\n",
    "# Normalize pixel values to range [0, 1]\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0\n",
    "\n",
    "# Print shapes\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train_int.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09711e33-2a6d-4552-8391-7ac96d303d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(height, width, 3)),\n",
    "    MaxPooling2D((2,2)),\n",
    "\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D((2,2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(20, activation='softmax')  #the 20 different categories for age \n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24bcd747-3ff4-42d0-a20b-bc46fe1cdc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "226/226 [==============================] - 58s 249ms/step - loss: 32.6990 - accuracy: 0.1618 - val_loss: 2.5594 - val_accuracy: 0.1656\n",
      "Epoch 2/5\n",
      "226/226 [==============================] - 56s 246ms/step - loss: 2.3848 - accuracy: 0.2340 - val_loss: 2.6238 - val_accuracy: 0.1532\n",
      "Epoch 3/5\n",
      "226/226 [==============================] - 55s 245ms/step - loss: 2.0607 - accuracy: 0.3447 - val_loss: 2.9376 - val_accuracy: 0.1706\n",
      "Epoch 4/5\n",
      "226/226 [==============================] - 55s 241ms/step - loss: 1.8192 - accuracy: 0.4082 - val_loss: 3.4519 - val_accuracy: 0.1519\n",
      "Epoch 5/5\n",
      "226/226 [==============================] - 55s 244ms/step - loss: 1.4664 - accuracy: 0.5361 - val_loss: 3.8658 - val_accuracy: 0.1681\n",
      "1004/1004 [==============================] - 75s 75ms/step - loss: 2.4476 - accuracy: 0.1766\n",
      "Test Loss: 2.4476, Test Accuracy: 0.1766\n",
      "1/1 [==============================] - 1s 816ms/step\n",
      "\n",
      "Sample Predictions:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'int_to_label_mapping' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSample Predictions:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m---> 19\u001b[0m     predicted_label \u001b[38;5;241m=\u001b[39m \u001b[43mint_to_label_mapping\u001b[49m[pred_classes[i]]\n\u001b[0;32m     20\u001b[0m     actual_label \u001b[38;5;241m=\u001b[39m int_to_label_mapping[y_test_int[i]]\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Actual: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactual_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'int_to_label_mapping' is not defined"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train_int,\n",
    "    validation_split=0.1,  # 10% of training as validation\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test_int, verbose=1)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "predictions = model.predict(X_test[:5])\n",
    "pred_classes = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14bd8eaa-2c60-4279-b037-e6759911f9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Predictions:\n",
      "Predicted: 20-25 years, Actual: 25-30 years\n",
      "Predicted: 20-25 years, Actual: 20-25 years\n",
      "Predicted: 20-25 years, Actual: 35-40 years\n",
      "Predicted: 20-25 years, Actual: 20-25 years\n",
      "Predicted: 25-30 years, Actual: 20-25 years\n"
     ]
    }
   ],
   "source": [
    "int_to_label = {v: k for k, v in label_to_int.items()}\n",
    "\n",
    "print(\"\\nSample Predictions:\")\n",
    "for i in range(5):\n",
    "    predicted_label = int_to_label[pred_classes[i]]\n",
    "    actual_label = int_to_label[y_test_int[i]]\n",
    "    print(f\"Predicted: {predicted_label}, Actual: {actual_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bce1d76-1e6b-449c-a810-51354f1126f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
