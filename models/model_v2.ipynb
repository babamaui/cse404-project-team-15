{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e040ec1-8705-4f3e-a020-dc08409d03b8",
   "metadata": {},
   "source": [
    "## Same intial loading and cleaning steps ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4147e45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preparation Imports\n",
    "import cv2\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "file_name = '../data/wiki.mat'\n",
    "\n",
    "def matlab_datenum_to_date(matlab_dn):\n",
    "    return datetime.date.fromordinal(int(matlab_dn) - 366)\n",
    "\n",
    "def compute_wiki_age(photo_year, birth_datenum):\n",
    "    photo_date = datetime.date(photo_year, 7, 1)\n",
    "    birth_date = matlab_datenum_to_date(birth_datenum)\n",
    "    delta = photo_date - birth_date\n",
    "    return delta.days / 365.2425\n",
    "\n",
    "\n",
    "def process_matfile(file_name):\n",
    "    # Load the data\n",
    "    data = loadmat(file_name)['wiki'][0, 0]\n",
    "    data_dict = {key: data[key] for key in data.dtype.names}\n",
    "\n",
    "    # Convert data into filename-based dictionary\n",
    "    filename_dict = {}\n",
    "\n",
    "    for i in range(len(data_dict['full_path'][0])):\n",
    "        # Remove invalid faces\n",
    "        if np.isinf(data_dict['face_score'][0][i]) or not np.isnan(data_dict['second_face_score'][0][i]):\n",
    "            continue\n",
    "\n",
    "        filename = data_dict['full_path'][0][i][0]\n",
    "        filename_dict[filename] = compute_wiki_age(data_dict['photo_taken'][0][i], data_dict['dob'][0][i])\n",
    "\n",
    "    return filename_dict\n",
    "\n",
    "# Load filenames anda ssociated data\n",
    "data = process_matfile(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de848d31-42b3-4271-b295-9986f432db6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40216\n",
      "38.60996556567274\n",
      "40147\n",
      "1.3525260614523227 99.87337180092678\n"
     ]
    }
   ],
   "source": [
    "#some data cleaning to get rid of excessively large or negative values in the predicted age category\n",
    "print(len(data))\n",
    "mean_val = sum(data.values()) / len(data)\n",
    "print(mean_val)\n",
    "for key in list(data.keys()):\n",
    "    if data[key] > 100 or data[key] < 1:\n",
    "        del data[key]\n",
    "print(len(data))\n",
    "print(min(data.values()), max(data.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52c154cb-9def-4a6c-a7c5-d748c77d8d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images loaded: 40147\n"
     ]
    }
   ],
   "source": [
    "# Load in images\n",
    "images = []\n",
    "for i in data.keys():  \n",
    "    img = cv2.imread('../data/wiki_crop/'+i)\n",
    "    images.append(img)\n",
    "\n",
    "print(f\"Number of images loaded: {len(images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c1beabd-ca4a-4268-ab39-313b827a6e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resize all images (can change width and height to what we find to be best)\n",
    "new_images = []\n",
    "width = 200\n",
    "height = 200\n",
    "for image in images:\n",
    "    new_im = cv2.resize(image, (width,height), interpolation=cv2.INTER_LINEAR)\n",
    "    new_images.append(new_im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb9d6ee-68d7-4cd8-89d6-7181abe38f76",
   "metadata": {},
   "source": [
    "## new way to classify ages into bins of 5 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9483bb2-6f04-4b98-b0bb-63df96f5b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]\n",
    "labels = ['0-5 years', '5-10 years', '10-15 years', '15-20 years',\n",
    "              '20-25 years', '25-30 years', '30-35 years', '35-40 years',\n",
    "             '40-45 years', '45-50 years', '50-55 years', '55-60 years',\n",
    "              '60-65 years', '65-70 years', '70-75 years', '75-80 years', \n",
    "             '80-85 years', '85-90 years', '90-95 years', '95-100 years']\n",
    "\n",
    "def classify_age(age, edges, labels):\n",
    "    for i in range(len(edges) - 1):\n",
    "        if edges[i] <= age < edges[i + 1]:\n",
    "            return labels[i]\n",
    "    return None  \n",
    "\n",
    "classification = [classify_age(data[key], edges, labels) for key in data.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0964e931-582f-4e40-8170-c7a97fba9394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in to train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(new_images, classification, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "331dfa63-0271-4266-9279-54c55be42e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (32117, 200, 200, 3)\n",
      "y_train shape: (32117,)\n"
     ]
    }
   ],
   "source": [
    "label_to_int_mapping = {label: idx for idx, label in enumerate(labels)}\n",
    "\n",
    "# Function to map labels to integers\n",
    "def label_to_int(label):\n",
    "    return label_to_int_mapping[label]\n",
    "\n",
    "# Convert datasets\n",
    "X_train = np.array(x_train, dtype='float32')\n",
    "X_test = np.array(x_test, dtype='float32')\n",
    "\n",
    "# Map labels to integers\n",
    "y_train_int = np.array([label_to_int(lbl) for lbl in y_train], dtype='int32')\n",
    "y_test_int = np.array([label_to_int(lbl) for lbl in y_test], dtype='int32')\n",
    "\n",
    "# Normalize pixel values to range [0, 1]\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0\n",
    "\n",
    "# Print shapes\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train_int.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09711e33-2a6d-4552-8391-7ac96d303d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(height, width, 3)),\n",
    "    MaxPooling2D((2,2)),\n",
    "\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D((2,2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(20, activation='softmax')  #the 20 different categories for age \n",
    "])\n",
    "\n",
    "model2.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24bcd747-3ff4-42d0-a20b-bc46fe1cdc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "904/904 [==============================] - 222s 241ms/step - loss: 2.4412 - accuracy: 0.1850 - val_loss: 2.3254 - val_accuracy: 0.1803\n",
      "Epoch 2/10\n",
      "904/904 [==============================] - 437s 483ms/step - loss: 2.2249 - accuracy: 0.2202 - val_loss: 2.2643 - val_accuracy: 0.2080\n",
      "Epoch 3/10\n",
      "904/904 [==============================] - 218s 241ms/step - loss: 1.9678 - accuracy: 0.3171 - val_loss: 2.3635 - val_accuracy: 0.2070\n",
      "Epoch 4/10\n",
      "904/904 [==============================] - 215s 238ms/step - loss: 1.4524 - accuracy: 0.4955 - val_loss: 2.8175 - val_accuracy: 0.1915\n",
      "Epoch 5/10\n",
      "904/904 [==============================] - 219s 242ms/step - loss: 0.7618 - accuracy: 0.7434 - val_loss: 4.1811 - val_accuracy: 0.1743\n",
      "Epoch 6/10\n",
      "904/904 [==============================] - 216s 239ms/step - loss: 0.3111 - accuracy: 0.9031 - val_loss: 5.7516 - val_accuracy: 0.1719\n",
      "Epoch 7/10\n",
      "904/904 [==============================] - 232s 256ms/step - loss: 0.1446 - accuracy: 0.9608 - val_loss: 7.0974 - val_accuracy: 0.1597\n",
      "Epoch 8/10\n",
      "904/904 [==============================] - 216s 239ms/step - loss: 0.0909 - accuracy: 0.9782 - val_loss: 8.4493 - val_accuracy: 0.1594\n",
      "Epoch 9/10\n",
      "904/904 [==============================] - 215s 237ms/step - loss: 0.0956 - accuracy: 0.9758 - val_loss: 9.0741 - val_accuracy: 0.1491\n",
      "Epoch 10/10\n",
      "904/904 [==============================] - 215s 238ms/step - loss: 0.0831 - accuracy: 0.9794 - val_loss: 9.5067 - val_accuracy: 0.1603\n",
      "251/251 [==============================] - 19s 74ms/step - loss: 9.3749 - accuracy: 0.1781\n",
      "Test Loss: 9.3749, Test Accuracy: 0.1781\n",
      "1/1 [==============================] - 1s 937ms/step\n",
      "\n",
      "Sample Predictions:\n",
      "Predicted: 20-25 years, Actual: 25-30 years\n",
      "Predicted: 30-35 years, Actual: 20-25 years\n",
      "Predicted: 35-40 years, Actual: 35-40 years\n",
      "Predicted: 50-55 years, Actual: 20-25 years\n",
      "Predicted: 20-25 years, Actual: 20-25 years\n"
     ]
    }
   ],
   "source": [
    "history = model2.fit(\n",
    "    X_train, y_train_int,\n",
    "    validation_split=0.1,  # 10% of training as validation\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "test_loss, test_acc = model2.evaluate(X_test, y_test_int, verbose=1)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "predictions = model2.predict(X_test[:5])\n",
    "pred_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "int_to_label = {v: k for k, v in label_to_int_mapping.items()}\n",
    "\n",
    "print(\"\\nSample Predictions:\")\n",
    "for i in range(5):\n",
    "    predicted_label = int_to_label[pred_classes[i]]\n",
    "    actual_label = int_to_label[y_test_int[i]]\n",
    "    print(f\"Predicted: {predicted_label}, Actual: {actual_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc9ddc4-a4aa-4cfe-aef6-2eb86129f522",
   "metadata": {},
   "source": [
    "## New attempt ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bce1d76-1e6b-449c-a810-51354f1126f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification = [int(data[key]) for key in data.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63246684-4e8f-4c54-80d7-c3df964ee357",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(new_images, classification, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2c60336-c64c-41b3-9450-eb5dffc00d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train = np.array(y_train, dtype='float32') \n",
    "y_test  = np.array(y_test, dtype='float32')\n",
    "\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80fe6077-198e-4008-9943-65775a104cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_2 = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(height, width, 3)),\n",
    "    MaxPooling2D((2,2)),\n",
    "\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D((2,2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(100, activation='softmax')  #the 100 different ages \n",
    "])\n",
    "\n",
    "model2_2.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22fa1fb1-499f-40fd-a189-a746be4b95a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "904/904 [==============================] - 224s 246ms/step - loss: 4.0795 - accuracy: 0.0391 - val_loss: 4.0678 - val_accuracy: 0.0370\n",
      "Epoch 2/5\n",
      "904/904 [==============================] - 217s 240ms/step - loss: 4.0539 - accuracy: 0.0395 - val_loss: 4.0518 - val_accuracy: 0.0361\n",
      "Epoch 3/5\n",
      "904/904 [==============================] - 217s 240ms/step - loss: 4.0482 - accuracy: 0.0380 - val_loss: 4.0534 - val_accuracy: 0.0392\n",
      "Epoch 4/5\n",
      "904/904 [==============================] - 219s 243ms/step - loss: 4.0466 - accuracy: 0.0396 - val_loss: 4.0507 - val_accuracy: 0.0370\n",
      "Epoch 5/5\n",
      "904/904 [==============================] - 218s 241ms/step - loss: 4.0464 - accuracy: 0.0392 - val_loss: 4.0523 - val_accuracy: 0.0392\n",
      "251/251 [==============================] - 19s 75ms/step - loss: 4.0283 - accuracy: 0.0437\n",
      "Test Loss: 4.0283\n",
      "251/251 [==============================] - 17s 66ms/step\n",
      "Custom Accuracy (within 5 years): 0.3910\n",
      "\n",
      "Sample Predictions:\n",
      "Predicted: 26, Actual: 25.0, Within 5 years: Yes\n",
      "Predicted: 26, Actual: 22.0, Within 5 years: Yes\n",
      "Predicted: 26, Actual: 37.0, Within 5 years: No\n",
      "Predicted: 26, Actual: 20.0, Within 5 years: No\n",
      "Predicted: 26, Actual: 22.0, Within 5 years: Yes\n"
     ]
    }
   ],
   "source": [
    "history = model2_2.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.1,  # 10% of training as validation\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model (loss will still be calculated as usual)\n",
    "test_loss, _ = model2_2.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = model2_2.predict(X_test)\n",
    "pred_classes = np.argmax(predictions, axis=1)  # Predicted classes\n",
    "\n",
    "# Custom accuracy calculation (within 5 years)\n",
    "within_5_years = np.abs(pred_classes - y_test) <= 5\n",
    "custom_accuracy = np.mean(within_5_years)  # Fraction of correct predictions\n",
    "\n",
    "print(f\"Custom Accuracy (within 5 years): {custom_accuracy:.4f}\")\n",
    "\n",
    "# Sample predictions\n",
    "print(\"\\nSample Predictions:\")\n",
    "for i in range(5):\n",
    "    print(f\"Predicted: {pred_classes[i]}, Actual: {y_test[i]}, \"\n",
    "          f\"Within 5 years: {'Yes' if within_5_years[i] else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295865f7-b7f8-4bf8-bbea-0824792b42a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSE404Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
